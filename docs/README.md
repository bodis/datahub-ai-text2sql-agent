# AI Data Agent Documentation

**Current Version:** v1.0
**Last Updated:** 2025-10-27

## Overview

AI Data Agent is an intelligent database query system that uses a multi-stage LLM pipeline to understand natural language questions and execute SQL queries across multiple databases. It features agentic SQL generation with automatic error recovery, token tracking, and a real-time chat interface.

---

## ğŸ¯ Current State

### âœ… Fully Implemented Features

#### 1. **Multi-Stage LLM Pipeline**
- **Validation Stage** (Haiku 4): Determines if questions are relevant
- **Decision Stage** (Sonnet 4.5): Decides action (clarify, plan, answer, reject)
- **Planning Stage** (Sonnet 4.5): Creates step-by-step query plans
- **Execution Stage** (Sonnet 4.5): Generates and executes SQL with retry logic
- **Summary Stage** (Sonnet 4.5): Synthesizes results into natural language answers

#### 2. **Agentic SQL Execution**
- Automatic SQL generation for each plan step
- Error detection and analysis
- Up to 5 retry attempts per step with error correction
- Distinguishes recoverable vs non-recoverable errors
- Passes previous step results to subsequent steps

#### 3. **Database Architecture**
- 6 separate PostgreSQL databases (customer, accounts, loans, insurance, compliance, employees)
- One datasource per database for proper isolation
- Schema information loaded from YAML files
- Cross-database queries handled through multi-step execution

#### 4. **Token Tracking**
- Per-thread token usage tracking
- Real-time display in UI
- Tracks input/output tokens and total calls
- Aggregates across all LLM interactions

#### 5. **Database Usage Visualization**
- Tracks which databases are used in each thread
- Visual highlighting in UI (used = colorful, unused = dimmed)
- Automatic refresh after each message
- Thread-specific tracking

#### 6. **Debug Logging**
- Full prompt logging when `LLM_DEBUG=true`
- Complete system prompts and user messages
- Structured response logging
- Token usage and timing information

#### 7. **Error Recovery**
- Categorizes errors: syntax, schema, permission, connection, data
- Automatic SQL correction for recoverable errors
- Graceful handling of non-recoverable errors
- Detailed error messages to users

---

## ğŸ—ï¸ Architecture

### High-Level Flow

```
User Question
    â†“
Validation (Haiku)
    â†“
Decision (Sonnet)
    â†“
Planning (Sonnet)
    â†“
For each step:
    Generate SQL (Sonnet)
    â†“
    Execute â†’ Error? â†’ Analyze & Fix â†’ Retry (up to 5x)
    â†“
    Success â†’ Next Step
    â†“
Summary (Sonnet)
    â†“
Natural Language Answer
```

### Technology Stack

**Backend:**
- Flask 3.0+ with uvicorn (ASGI)
- Python 3.10+
- Anthropic Claude API (Haiku 4 + Sonnet 4.5)
- PostgreSQL with psycopg3
- YAML-based configuration
- Pydantic for structured outputs

**Frontend:**
- React 18 + TypeScript
- Vite build tool
- Tailwind CSS + shadcn/ui
- Real-time UI updates

### Project Structure

```
ai_data_agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Flask app
â”‚   â”‚   â”œâ”€â”€ routes.py             # API endpoints
â”‚   â”‚   â”œâ”€â”€ storage.py            # In-memory storage
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py         # Claude client
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py        # Pydantic models
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py        # Prompt loader
â”‚   â”‚   â”‚   â”œâ”€â”€ orchestrator.py   # Pipeline orchestrator
â”‚   â”‚   â”‚   â””â”€â”€ executor.py       # Agentic SQL executor
â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â”‚       â”œâ”€â”€ base.py           # Abstract interface
â”‚   â”‚       â”œâ”€â”€ postgresql.py     # PostgreSQL impl
â”‚   â”‚       â””â”€â”€ manager.py        # Datasource router
â”‚   â”œâ”€â”€ knowledge/
â”‚   â”‚   â”œâ”€â”€ data_schemas/         # Database schemas (YAML)
â”‚   â”‚   â”œâ”€â”€ prompts/              # LLM prompts (YAML)
â”‚   â”‚   â”œâ”€â”€ docs/                 # Documentation
â”‚   â”‚   â””â”€â”€ datasources.yaml      # Datasource config
â”‚   â”œâ”€â”€ .env                      # Environment config
â”‚   â””â”€â”€ pyproject.toml            # Dependencies
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/           # React components
â”‚       â”œâ”€â”€ lib/                  # API client
â”‚       â””â”€â”€ App.tsx               # Main app
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture/             # Architecture docs
â”‚   â”œâ”€â”€ implementation/           # Implementation details
â”‚   â””â”€â”€ guides/                   # How-to guides
â””â”€â”€ CLAUDE.md                     # Project context
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- PostgreSQL (optional, for real databases)
- Anthropic API key

### Backend Setup

```bash
cd backend

# Create .env file
cp .env.example .env

# Add your API key to .env
# ANTHROPIC_API_KEY=your_key_here

# Install dependencies
uv sync

# Start server
uv run uvicorn main:app --host 0.0.0.0 --port 5001 --reload
```

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
```

### Access

- Frontend: http://localhost:3001
- Backend API: http://localhost:5001

---

## ğŸ“– API Endpoints

### Threads

- `GET /api/threads` - List all threads
- `POST /api/threads` - Create new thread
- `GET /api/threads/:id` - Get thread details
- `GET /api/threads/:id/messages` - Get thread messages
- `POST /api/threads/:id/messages` - Send message (triggers LLM)
- `GET /api/threads/:id/tokens` - Get token usage
- `GET /api/threads/:id/databases` - Get used databases

### Data Sources

- `GET /api/data-sources` - Get available databases

---

## ğŸ¨ UI Features

### Database Badges

Visual indicators showing which databases are available and which are being used:

- **Before first message**: All badges dimmed (gray, 30% opacity)
- **After messages**: Used databases highlighted (colorful), unused remain dimmed
- **Hover tooltip**: Shows database description and usage status

### Token Display

Real-time token usage per thread:
- Input tokens
- Output tokens
- Total tokens
- Number of LLM calls

### Debug Panel

Expandable panel showing:
- Each LLM interaction
- Model used
- Token counts
- Response times
- Pipeline stages

---

## ğŸ”§ Configuration

### Environment Variables

**Required:**
```bash
ANTHROPIC_API_KEY=your_key_here
```

**Model Configuration:**
```bash
ANTHROPIC_WEAK_MODEL=claude-haiku-4-5
ANTHROPIC_PLANNING_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_DEVELOPER_MODEL=claude-sonnet-4-5-20250929
```

**Debug:**
```bash
LLM_DEBUG=true  # Enable full prompt/response logging
```

**PostgreSQL (if using real databases):**
```bash
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

POSTGRES_CUSTOMER_DB=customer_db
POSTGRES_ACCOUNTS_DB=accounts_db
POSTGRES_LOANS_DB=loans_db
POSTGRES_INSURANCE_DB=insurance_db
POSTGRES_COMPLIANCE_DB=compliance_db
POSTGRES_EMPLOYEES_DB=employees_db
```

### Database Schemas

All database schemas are defined in YAML files at:
```
backend/knowledge/data_schemas/
â”œâ”€â”€ summary.yaml          # High-level overview
â”œâ”€â”€ customer_db.yaml      # Customer database schema
â”œâ”€â”€ accounts_db.yaml      # Accounts database schema
â”œâ”€â”€ loans_db.yaml         # Loans database schema
â”œâ”€â”€ insurance_db.yaml     # Insurance database schema
â”œâ”€â”€ compliance_db.yaml    # Compliance database schema
â””â”€â”€ employees_db.yaml     # Employees database schema
```

### LLM Prompts

All prompts are in YAML format at:
```
backend/knowledge/prompts/
â”œâ”€â”€ validate_question.yaml  # Validation stage
â”œâ”€â”€ decide_action.yaml      # Decision stage
â”œâ”€â”€ create_plan.yaml        # Planning stage
â”œâ”€â”€ generate_sql.yaml       # SQL generation
â”œâ”€â”€ analyze_error.yaml      # Error analysis
â””â”€â”€ write_summary.yaml      # Summary generation
```

---

## ğŸ“š Documentation Index

### Architecture
- [System Architecture](./architecture/SYSTEM_ARCHITECTURE.md) - Overall system design
- [Datasource Architecture](./architecture/DATASOURCE_ARCHITECTURE.md) - Database connection model
- [LLM Pipeline](./architecture/LLM_PIPELINE.md) - Multi-stage pipeline details

### Implementation
- [Agentic Execution](./implementation/AGENTIC_EXECUTION.md) - SQL generation and retry logic
- [Database Tracking](./implementation/DATABASE_TRACKING.md) - Usage tracking implementation
- [Error Recovery](./implementation/ERROR_RECOVERY.md) - Error handling strategies
- [Schema Management](./implementation/SCHEMA_MANAGEMENT.md) - Schema loading and usage

### Guides
- [Getting Started](./guides/GETTING_STARTED.md) - Complete setup guide
- [Adding Databases](./guides/ADDING_DATABASES.md) - How to add new databases
- [Customizing Prompts](./guides/CUSTOMIZING_PROMPTS.md) - Prompt engineering guide
- [Troubleshooting](./guides/TROUBLESHOOTING.md) - Common issues and solutions

---

## ğŸ” Key Concepts

### Agentic Execution

Each SQL query is generated and executed through an agentic loop:

1. **Generate SQL**: LLM creates query based on step requirements
2. **Execute**: Run query through datasource manager
3. **Evaluate**: Check for errors
4. **Recover**: If error is recoverable, generate fix and retry
5. **Iterate**: Up to 5 attempts per step

### Error Types

- **Syntax**: SQL syntax errors (recoverable)
- **Schema**: Wrong table/column names (recoverable)
- **Data**: Type mismatches, constraint violations (may be recoverable)
- **Permission**: Access denied (not recoverable)
- **Connection**: Database unavailable (not recoverable)

### Multi-Step Queries

Cross-database queries are handled through sequential steps:

**Example: "Show customers with their account balances"**
- Step 1: Query `customer_db` for customer data
- Step 2: Query `accounts_db` for balances using customer IDs from Step 1
- Step 3: Summary combines results into natural language answer

### Token Optimization

- Validation uses lightweight Haiku model (~500 tokens)
- SQL generation only includes relevant schemas
- Error recovery sends only last attempt (not full history)
- Result: ~37% reduction in error analysis tokens

---

## ğŸ¯ Best Practices

### Writing Questions

**Good:**
- "How many customers do we have?"
- "Show me the top 5 loans by amount"
- "Which customers filed complaints in Q4?"

**Avoid:**
- Very vague questions without context
- Questions spanning too many databases at once
- Questions about data not in the system

### Database Design

- Keep schemas well-documented in YAML
- Use clear table and column names
- Define foreign key relationships
- Include sample values for enums

### Prompt Engineering

- Use clear, specific instructions
- Provide examples for complex scenarios
- Specify output format requirements
- Handle edge cases explicitly

---

## ğŸ› Known Limitations

1. **No cross-database JOINs**: PostgreSQL limitation, handled through multi-step execution
2. **In-memory storage**: Data lost on restart (can be replaced with persistent storage)
3. **No query caching**: Each query is fresh (could add Redis)
4. **Token costs**: Complex queries use significant tokens (optimization ongoing)
5. **No streaming**: Responses come all at once (could add SSE)

---

## ğŸš§ Future Enhancements

### Planned Features

- [ ] Persistent storage (PostgreSQL for threads/messages)
- [ ] Query result caching
- [ ] Streaming responses (SSE)
- [ ] Query history and favorites
- [ ] Export results (CSV, JSON, Excel)
- [ ] Advanced visualizations (charts, graphs)
- [ ] Role-based access control
- [ ] Audit logging
- [ ] Query optimization suggestions
- [ ] Natural language to SQL training mode

### Potential Improvements

- Parallel execution of independent steps
- Query plan optimization
- Custom retry strategies per error type
- Incremental result streaming
- SQL validation before execution
- Cost estimation before query execution

---

## ğŸ“Š Performance

### Typical Response Times

- **Simple query** (1 database, 1 step): 2-4 seconds
- **Complex query** (2 databases, 3 steps): 6-10 seconds
- **With retries** (SQL errors): +2-3 seconds per retry

### Token Usage (Approximate)

- **Validation**: 400-600 tokens
- **Decision**: 500-800 tokens
- **Planning**: 1,000-2,000 tokens
- **SQL generation per step**: 1,500-3,000 tokens
- **Error analysis per retry**: 1,000-2,000 tokens
- **Summary**: 1,500-3,000 tokens

**Total for complex query**: 10,000-20,000 tokens

---

## ğŸ¤ Contributing

See individual implementation documents for details on:
- Adding new datasources
- Creating custom prompts
- Extending the storage layer
- Adding new LLM stages

---

## ğŸ“ License

[Add your license here]

---

## ğŸ†˜ Support

For issues and questions:
1. Check the [Troubleshooting Guide](./guides/TROUBLESHOOTING.md)
2. Review [CLAUDE.md](../CLAUDE.md) for project context
3. Examine debug logs with `LLM_DEBUG=true`

---

**Last Updated:** 2025-10-27
**Version:** 1.0
**Status:** Production Ready âœ…
